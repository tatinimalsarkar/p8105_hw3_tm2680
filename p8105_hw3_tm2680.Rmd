---
title: "p8105_hw3_tm2680"
author: "Tatini Mal-Sarkar"
date: "10/4/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
devtools::install_github("p8105/p8105.datasets")
library(ggridges)
library(lubridate)
library(p8105.datasets)
```

# Problem 1
```{r brfss_clean}
brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  select(-class, -topic, -question, -data_value_footnote_symbol:-respid) %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent", ordered = TRUE))) 
```

```{r brfss_seven_loc}
brfss_data %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarize(n = n_distinct(geo_location)) %>% 
  filter(n == 7)
```

In 2002, Connecticut, Florida, and North Carolina were all observed at 7 distinct locations in the `brfss_data` dataset.

```{r brfss_spag_plot}
brfss_data %>% 
  group_by(locationabbr, year) %>% 
  summarize(n = n_distinct(geo_location)) %>% 
  ggplot(aes(x = year, y = n, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "bottom")
```

Here is my spaghetti plot showing the number of locations across time for the `brfss_data` dataset. The colors correspond to state, and the data is from 2002 to 2010. 

```{r brfss_tab_exc}
brfss_data %>% 
  filter(locationabbr == "NY") %>% 
  group_by(year) %>% 
  spread(response, data_value) %>% 
  janitor::clean_names() %>% 
  filter(year == 2002 | year == 2006 | year == 2010) %>% 
  summarize(mean_exc = mean(excellent, na.rm = TRUE),
            sd_exc = sd(excellent, na.rm = TRUE)) 
```

This is my table that shows the mean and standard deviation for the proportion of "Excellent" responses across all locations in New York for the years 2002, 2006, and 2010. 

```{r brfss_prop_resp}
brfss_data %>% 
  group_by(year, locationabbr) %>% 
  spread(response, data_value) %>% 
  janitor::clean_names() %>% 
  summarize(mean_exc = mean(excellent, na.rm = TRUE),
            mean_vgood = mean(very_good, na.rm = TRUE),
            mean_good = mean(good, na.rm = TRUE),
            mean_fair = mean(fair, na.rm = TRUE),
            mean_poor = mean(poor, na.rm = TRUE)) %>% 
  gather(response, means, mean_exc:mean_poor) %>% 
  ggplot(aes(x = year, y = means, color = locationabbr)) +
  geom_line() + 
  facet_grid( ~ response)
  theme(legend.position = "bottom")
```

Here is my 5-panel plot of distribution of state-level averages of the response types over time.

# Problem 2

The `instacart` dataset contains `r nrow(instacart)` observations and `r ncol(instacart)` variables. These variables describe orders from Instacart, a grocery service. For instance, `aisle` (e.g. fresh fruits) and `department`  (e.g. canned goods) refer to the aisle and department the specific product within that order comes from. Every product from each order is listed as its own observation. For example, `order_id` 1 has 8 observations, related to 8 different products. We assume that `order_dow` = 0 corresponds to Sunday.

```{r insta_expl}
instacart %>% 
  janitor::clean_names() %>%
  group_by(aisle_id) %>% 
  summarize(n_aisle = n()) %>% 
  filter(min_rank(desc(n_aisle)) < 2)
```

The `instacart` dataset contains 134 aisles. The aisle with the most items ordered was aisle 83.

```{r insta_item_plot}
instacart %>% 
  janitor::clean_names() %>% 
  group_by(aisle_id) %>% 
  summarize(number_of_items = n()) %>%
  ggplot(aes(x = aisle_id, y = number_of_items)) + 
  geom_bar(stat = "identity") +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle",
    y = "Number of items ordered",
    caption = "Data from P8105 Instacart dataset"
  ) +
  theme(legend.position = "bottom")
```

Here is my plot of how many items are getting ordered in each aisle. It is a bar chart to appropriately and legibly show the relationship between aisle and items ordered. 

```{r insta_tab_pop}
instacart %>% 
  janitor::clean_names() %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_prod_ord = n()) %>% 
  filter(min_rank(desc(n_prod_ord)) < 2) %>% 
  select(-n_prod_ord) %>% 
  as_tibble()
```
Here is my table that shows the most popular item in each of the following aisles.

```{r insta_app_coff}
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(order_dow = order_dow + 1,
    day = lubridate::wday(order_dow, label = TRUE, abbr = TRUE, week_start = getOption("lubridate.week.start", 7)),
    hour = lubridate::hour(date_decimal(mean_hour)),
    hour = format(strptime(hour, format = "%H"), "%I %p")) %>% 
  select(-order_dow, -mean_hour) %>% 
  spread(day, hour)
```

# Problem 3

The `ny_noaa` dataset contains `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables. Each observation refers to a specific day at a specific location and provides data such as precipitation (`prcp`), snowfall (`snow`), and temperatures (maximum temperature is `tmax`; minimum temperature is `tmin`). Quite a bit of data is missing. 
Variable `prcp` contains `r sum(is.na(ny_noaa$prcp))` missing values. Variable `snow` contains  `r sum(is.na(ny_noaa$snow))` missing values. Variable `snwd` contains `r sum(is.na(ny_noaa$snwd))` missing values. Variable `tmax` contains `r sum(is.na(ny_noaa$tmax))` missing values. Variable `tmin` contains `r sum(is.na(ny_noaa$tmin))` missing values.

```{r noaa_clean}
noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  mutate(year = lubridate::year(date),
         month = lubridate::month(date),
         day = lubridate::day(date))
```

