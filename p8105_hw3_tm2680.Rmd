---
title: "p8105_hw3_tm2680"
author: "Tatini Mal-Sarkar"
date: "10/4/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
devtools::install_github("p8105/p8105.datasets")
library(ggridges)
library(lubridate)
library(patchwork)
library(p8105.datasets)
```

# Problem 1
```{r brfss_clean}
brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  select(-class, -topic, -question, -data_value_footnote_symbol:-respid) %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent", ordered = TRUE))) 
```

```{r brfss_seven_loc}
brfss_data %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarize(n = n_distinct(geo_location)) %>% 
  filter(n == 7)
```

In 2002, Connecticut, Florida, and North Carolina were all observed at 7 distinct locations in the `brfss_data` dataset.

```{r brfss_spag_plot}
brfss_data %>% 
  group_by(locationabbr, year) %>% 
  summarize(n = n_distinct(geo_location)) %>% 
  ggplot(aes(x = year, y = n, color = locationabbr)) + 
  geom_line() +
  labs(
    title = "Spaghetti plot",
    x = "Year",
    y = "Number of locations in each state",
    caption = "Data from BRFSS"
  ) +
  scale_color_hue(name = "State")
```

Here is my spaghetti plot showing the number of locations across time for the `brfss_data` dataset. The colors correspond to state, and the data is from 2002 to 2010. 

```{r brfss_tab_exc}
brfss_data %>% 
  filter(locationabbr == "NY") %>% 
  group_by(year) %>% 
  spread(response, data_value) %>% 
  janitor::clean_names() %>% 
  filter(year == 2002 | year == 2006 | year == 2010) %>% 
  summarize(mean_exc = mean(excellent, na.rm = TRUE),
            sd_exc = sd(excellent, na.rm = TRUE)) %>% 
  knitr::kable()
```

This is my table that shows the mean and standard deviation for the proportion of "Excellent" responses across all locations in New York for the years 2002, 2006, and 2010. 

```{r brfss_prop_resp}
brfss_data %>% 
  group_by(year, locationabbr) %>% 
  spread(response, data_value) %>% 
  janitor::clean_names() %>% 
  summarize(mean_exc = mean(excellent, na.rm = TRUE),
            mean_vgood = mean(very_good, na.rm = TRUE),
            mean_good = mean(good, na.rm = TRUE),
            mean_fair = mean(fair, na.rm = TRUE),
            mean_poor = mean(poor, na.rm = TRUE)) %>% 
  gather(response, means, mean_exc:mean_poor) %>% 
  ggplot(aes(x = year, y = means, color = locationabbr)) +
  geom_line() + 
  facet_grid( ~ response) +
  scale_color_hue(name = "State") + 
  theme(axis.text.x = element_text(angle = 90))
```

Here is my 5-panel plot of distribution of state-level averages of the response types over time.

# Problem 2

The `instacart` dataset contains `r nrow(instacart)` observations and `r ncol(instacart)` variables. These variables describe orders from Instacart, a grocery service. For instance, `aisle` (e.g. fresh fruits) and `department`  (e.g. canned goods) refer to the aisle and department the specific product within that order comes from. Every product from each order is listed as its own observation. For example, `order_id` 1 has 8 observations, related to 8 different products. We assume that `order_dow` = 6 corresponds to Sunday.

```{r insta_expl}
instacart %>% 
  janitor::clean_names() %>%
  group_by(aisle_id) %>% 
  summarize(n_aisle = n()) %>% 
  filter(min_rank(desc(n_aisle)) < 2)
```

The `instacart` dataset contains 134 aisles. The aisle with the most items ordered was aisle 83.

```{r insta_item_plot}
instacart %>% 
  janitor::clean_names() %>% 
  group_by(aisle_id) %>% 
  summarize(number_of_items = n()) %>%
  ggplot(aes(x = aisle_id, y = number_of_items)) + 
  geom_bar(stat = "identity") +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle",
    y = "Number of items ordered",
    caption = "Data from P8105 Instacart dataset"
  ) +
  scale_x_continuous(breaks = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140),
                     labels = c("0", "10", "20", "30", "40", "50", "60", "70", "80", "90", "100", "110", "120", "130", "140")) +
  theme(legend.position = "bottom")
```

Here is my plot of how many items are getting ordered in each aisle. It is a bar chart to appropriately and legibly show the relationship between aisle and items ordered. 

```{r insta_tab_pop}
instacart %>% 
  janitor::clean_names() %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_prod_ord = n()) %>% 
  filter(min_rank(desc(n_prod_ord)) < 2) %>% 
  select(-n_prod_ord) %>% 
  knitr::kable()
```

Here is my table that shows the most popular item in each of the following aisles.

```{r insta_app_coff}
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(order_dow = order_dow + 1,
    day = lubridate::wday(order_dow, label = TRUE, abbr = TRUE, week_start = getOption("lubridate.week.start", 7)),
    hour = lubridate::hour(date_decimal(mean_hour)),
    hour = format(strptime(hour, format = "%H"), "%I %p")) %>% 
  select(-order_dow, -mean_hour) %>% 
  spread(day, hour) %>% 
  knitr::kable()
```

# Problem 3

The `ny_noaa` dataset contains `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables. Each observation refers to a specific day at a specific location and provides data such as precipitation (`prcp`), snowfall (`snow`), and temperatures (maximum temperature is `tmax`; minimum temperature is `tmin`). Quite a bit of data is missing. Variable `prcp` contains `r sum(is.na(ny_noaa$prcp))` missing values. Variable `snow` contains  `r sum(is.na(ny_noaa$snow))` missing values. Variable `snwd` contains `r sum(is.na(ny_noaa$snwd))` missing values. Variable `tmax` contains `r sum(is.na(ny_noaa$tmax))` missing values. Variable `tmin` contains `r sum(is.na(ny_noaa$tmin))` missing values.

```{r noaa_clean}
noaa_clean = ny_noaa %>% 
  janitor::clean_names() %>% 
  mutate(tmin = as.numeric(tmin),
         tmax = as.numeric(tmax)) %>% 
  mutate(tmin = tmin / 10,
         tmax = tmax / 10,
         prcp = prcp / 10) %>% 
  mutate(year = lubridate::year(date),
         month = lubridate::month(date),
         day = lubridate::mday(date)) 
```

I made new variables for year, month, and day. Temperatures were provided in tenths of Celsius, which did not seem reasonable, so I converted `tmin` and `tmax` to Celsius. Precipitation was in tenths of millimeters, so I converted `prcp` to millimeters. 

```{r noaa_common_snow}
noaa_clean %>% 
  group_by(snow) %>% 
  summarize(common_snow = n()) %>% 
  filter(min_rank(desc(common_snow)) < 4) 
```

The most commonly observed values are 0mm, 25mm, and NA. NA is popular because there are a lot of missing entries. 0mm and 25mm are still quite low amounts of snowfall, but represent 0in and 1in in the metric system.

```{r noaa_max_plot}
noaa_clean %>% 
  filter(month == 1 | month == 7) %>% 
  group_by(month, year, id) %>% 
  summarize(mean_tmax = mean(tmax), na.rm = TRUE) %>% 
  ggplot(aes(x = year, y = mean_tmax)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth() +
  facet_grid( ~ month) +
  labs(
    title = "Average max temperature",
    x = "Year",
    y = "Mean max temperature (Celsius)",
    caption = "Data from NY NOAA dataset. 1 is January. 7 is July."
  )
```

Here is my plot showing average max temperature across years. Average max temperatures seem higher in July than January, which seems logical. There are some low outliers in both panels.

```{r noaa_tmax_tmin}
tmax_tmin_p = ggplot(noaa_clean, aes(x = tmin, y = tmax)) +
  geom_hex() +
  theme(legend.position = "bottom") +
  labs(
    x = "Minimum temperature (Celsius)",
    y = "Maximum temperature (Celsius)"
  )

snow_dist_p = noaa_clean %>% 
  filter(0 < snow & snow < 100) %>% 
  mutate(year = as.character(year)) %>% 
  ggplot(aes(x = snow, y = year)) +
  geom_density_ridges(scale = 0.85) +
  labs(
    x = "Snow (mm)",
    y = "Year"
  )

(tmax_tmin_p + snow_dist_p)
```

Here is my 2-panel plot. Panel 1 shows a hexplot of `tmax` versus `tmin` with all the data. Panel 2 uses a ridge plot to show the distribution of `snow` between 0 and 100 mm by `year`.