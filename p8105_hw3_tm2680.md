p8105\_hw3\_tm2680
================
Tatini Mal-Sarkar
10/4/2018

Problem 1
=========

``` r
brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  select(-class, -topic, -question, -data_value_footnote_symbol:-respid) %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent", ordered = TRUE))) 
```

``` r
brfss_data %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarize(n = n_distinct(geo_location)) %>% 
  filter(n == 7)
```

    ## # A tibble: 3 x 2
    ##   locationabbr     n
    ##   <chr>        <int>
    ## 1 CT               7
    ## 2 FL               7
    ## 3 NC               7

In 2002, Connecticut, Florida, and North Carolina were all observed at 7 distinct locations in the `brfss_data` dataset.

``` r
brfss_data %>% 
  group_by(locationabbr, year) %>% 
  summarize(n = n_distinct(geo_location)) %>% 
  ggplot(aes(x = year, y = n, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "bottom")
```

![](p8105_hw3_tm2680_files/figure-markdown_github/brfss_spag_plot-1.png)

Here is my spaghetti plot showing the number of locations across time for the `brfss_data` dataset. The colors correspond to state, and the data is from 2002 to 2010.

``` r
brfss_data %>% 
  filter(locationabbr == "NY") %>% 
  group_by(year) %>% 
  spread(response, data_value) %>% 
  janitor::clean_names() %>% 
  filter(year == 2002 | year == 2006 | year == 2010) %>% 
  summarize(mean_exc = mean(excellent, na.rm = TRUE),
            sd_exc = sd(excellent, na.rm = TRUE)) 
```

    ## # A tibble: 3 x 3
    ##    year mean_exc sd_exc
    ##   <int>    <dbl>  <dbl>
    ## 1  2002     24.0   4.49
    ## 2  2006     22.5   4.00
    ## 3  2010     22.7   3.57

This is my table that shows the mean and standard deviation for the proportion of "Excellent" responses across all locations in New York for the years 2002, 2006, and 2010.

``` r
brfss_data %>% 
  group_by(year, locationabbr) %>% 
  spread(response, data_value) %>% 
  janitor::clean_names() %>% 
  summarize(mean_exc = mean(excellent, na.rm = TRUE),
            mean_vgood = mean(very_good, na.rm = TRUE),
            mean_good = mean(good, na.rm = TRUE),
            mean_fair = mean(fair, na.rm = TRUE),
            mean_poor = mean(poor, na.rm = TRUE)) %>% 
  gather(response, means, mean_exc:mean_poor) %>% 
  ggplot(aes(x = year, y = means, color = locationabbr)) +
  geom_line() + 
  facet_grid( ~ response)
```

![](p8105_hw3_tm2680_files/figure-markdown_github/brfss_prop_resp-1.png)

``` r
  theme(legend.position = "bottom")
```

    ## List of 1
    ##  $ legend.position: chr "bottom"
    ##  - attr(*, "class")= chr [1:2] "theme" "gg"
    ##  - attr(*, "complete")= logi FALSE
    ##  - attr(*, "validate")= logi TRUE

Here is my 5-panel plot of distribution of state-level averages of the response types over time.

Problem 2
=========

The `instacart` dataset contains 1384617 observations and 15 variables. These variables describe orders from Instacart, a grocery service. For instance, `aisle` (e.g. fresh fruits) and `department` (e.g. canned goods) refer to the aisle and department the specific product within that order comes from. Every product from each order is listed as its own observation. For example, `order_id` 1 has 8 observations, related to 8 different products. We assume that `order_dow` = 0 corresponds to Sunday.

``` r
instacart %>% 
  janitor::clean_names() %>%
  group_by(aisle_id) %>% 
  summarize(n_aisle = n()) %>% 
  filter(min_rank(desc(n_aisle)) < 2)
```

    ## # A tibble: 1 x 2
    ##   aisle_id n_aisle
    ##      <int>   <int>
    ## 1       83  150609

The `instacart` dataset contains 134 aisles. The aisle with the most items ordered was aisle 83.

``` r
instacart %>% 
  janitor::clean_names() %>% 
  group_by(aisle_id) %>% 
  summarize(number_of_items = n()) %>%
  ggplot(aes(x = aisle_id, y = number_of_items)) + 
  geom_bar(stat = "identity") +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle",
    y = "Number of items ordered",
    caption = "Data from P8105 Instacart dataset"
  ) +
  theme(legend.position = "bottom")
```

![](p8105_hw3_tm2680_files/figure-markdown_github/insta_item_plot-1.png)

Here is my plot of how many items are getting ordered in each aisle. It is a bar chart to appropriately and legibly show the relationship between aisle and items ordered.

``` r
instacart %>% 
  janitor::clean_names() %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_prod_ord = n()) %>% 
  filter(min_rank(desc(n_prod_ord)) < 2) %>% 
  select(-n_prod_ord) %>% 
  as_tibble()
```

    ## # A tibble: 3 x 2
    ## # Groups:   aisle [3]
    ##   aisle                      product_name                                 
    ##   <chr>                      <chr>                                        
    ## 1 baking ingredients         Light Brown Sugar                            
    ## 2 dog food care              Snack Sticks Chicken & Rice Recipe Dog Treats
    ## 3 packaged vegetables fruits Organic Baby Spinach

Here is my table that shows the most popular item in each of the following aisles.

``` r
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(order_dow = order_dow + 1,
    day = lubridate::wday(order_dow, label = TRUE, abbr = TRUE, week_start = getOption("lubridate.week.start", 7)),
    hour = lubridate::hour(date_decimal(mean_hour)),
    hour = format(strptime(hour, format = "%H"), "%I %p")) %>% 
  select(-order_dow, -mean_hour) %>% 
  spread(day, hour)
```

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name     Sun   Mon   Tue   Wed   Thu   Fri   Sat  
    ##   <chr>            <chr> <chr> <chr> <chr> <chr> <chr> <chr>
    ## 1 Coffee Ice Cream 01 PM 06 AM 01 AM 03 AM 08 AM 07 AM 04 AM
    ## 2 Pink Lady Apples 12 AM 09 AM 06 AM 06 AM 09 AM 01 AM 04 AM

Problem 3
=========

The `ny_noaa` dataset contains 2595176 observations and 7 variables. Each observation refers to a specific day at a specific location and provides data such as precipitation (`prcp`), snowfall (`snow`), and temperatures (maximum temperature is `tmax`; minimum temperature is `tmin`). Quite a bit of data is missing. Variable `prcp` contains 145838 missing values. Variable `snow` contains 381221 missing values. Variable `snwd` contains 591786 missing values. Variable `tmax` contains 1134358 missing values. Variable `tmin` contains 1134420 missing values.

``` r
noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  mutate(year = lubridate::year(date),
         month = lubridate::month(date),
         day = lubridate::day(date))
```
